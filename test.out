====================
command:  tools/train_net.py --config-file projects/stabledino/configs/stabledino_r50_4scale_12ep.py --num-gpus 1 dataloader.train.total_batch_size=4 train.output_dir=./output/stabledino_r50_4scale_12ep train.test_with_nms=0.80 train.max_iter=150 train.total_batch_size=4
====================
[06/24 09:54:37 detectron2]: Rank of current process: 0. World size: 1
[06/24 09:54:38 detectron2]: Environment info:
----------------------  ------------------------------------------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]
numpy                   1.22.0
detectron2              0.6 @/fs/gpfs41/lv11/fileset01/pool/pool-lambacher/Cluster_Projects/detrex/detectron2/detectron2
Compiler                GCC 10.4
CUDA compiler           CUDA 12.1
detectron2 arch flags   /fs/gpfs41/lv11/fileset01/pool/pool-lambacher/Cluster_Projects/detrex/detectron2/detectron2/_C.cpython-310-x86_64-linux-gnu.so
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 2.3.1 @/fs/gpfs41/lv11/fileset01/pool/pool-lambacher/mambaforge/envs/detrex/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA H100 PCIe (arch=9.0)
Driver version          535.129.03
CUDA_HOME               None - invalid!
Pillow                  9.5.0
torchvision             0.18.1 @/fs/gpfs41/lv11/fileset01/pool/pool-lambacher/mambaforge/envs/detrex/lib/python3.10/site-packages/torchvision
torchvision arch flags  /fs/gpfs41/lv11/fileset01/pool/pool-lambacher/mambaforge/envs/detrex/lib/python3.10/site-packages/torchvision/_C.so
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.10.0
----------------------  ------------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.1-Product Build 20220311 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[06/24 09:54:38 detectron2]: Command line arguments: Namespace(config_file='projects/stabledino/configs/stabledino_r50_4scale_12ep.py', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:56599', opts=['dataloader.train.total_batch_size=4', 'train.output_dir=./output/stabledino_r50_4scale_12ep', 'train.test_with_nms=0.80', 'train.max_iter=150', 'train.total_batch_size=4'], comand_txt='tools/train_net.py --config-file projects/stabledino/configs/stabledino_r50_4scale_12ep.py --num-gpus 1 dataloader.train.total_batch_size=4 train.output_dir=./output/stabledino_r50_4scale_12ep train.test_with_nms=0.80 train.max_iter=150 train.total_batch_size=4')
[06/24 09:54:38 detectron2]: Contents of args.config_file=projects/stabledino/configs/stabledino_r50_4scale_12ep.py:
[38;5;204mfrom[39m[38;5;15m [39m[38;5;15mdetrex[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15mget_config[39m
[38;5;204mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mstabledino_r50[39m[38;5;15m [39m[38;5;204mimport[39m[38;5;15m [39m[38;5;15mmodel[39m

[38;5;245m# get default config[39m
[38;5;15mdataloader[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/data/coco_detr.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mdataloader[39m
[38;5;15moptimizer[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/optim.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mAdamW[39m
[38;5;15mlr_multiplier[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/coco_schedule.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mlr_multiplier_12ep[39m
[38;5;15mtrain[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/train.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;204m.[39m[38;5;15mtrain[39m

[38;5;245m# modify training config[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/torchvision/R-50.pkl[39m[38;5;186m"[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15moutput_dir[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./output/dino_r50_4scale_12ep[39m[38;5;186m"[39m

[38;5;245m# max training iterations[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mmax_iter[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m90000[39m

[38;5;245m# run evaluation every 5000 iters[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15meval_period[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m5000[39m

[38;5;245m# log training infomation every 20 iters[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m20[39m

[38;5;245m# set random seed[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mseed[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m60[39m

[38;5;245m# save checkpoint every 5000 iters[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mcheckpointer[39m[38;5;204m.[39m[38;5;15mperiod[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m5000[39m

[38;5;245m# gradient clipping for training[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mclip_grad[39m[38;5;204m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;81mTrue[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mclip_grad[39m[38;5;204m.[39m[38;5;15mparams[39m[38;5;204m.[39m[38;5;15mmax_norm[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mclip_grad[39m[38;5;204m.[39m[38;5;15mparams[39m[38;5;204m.[39m[38;5;15mnorm_type[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m2[39m

[38;5;245m# set training devices[39m
[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mdevice[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcuda[39m[38;5;186m"[39m
[38;5;15mmodel[39m[38;5;204m.[39m[38;5;15mdevice[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mdevice[39m

[38;5;245m# modify optimizer config[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m1e-4[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mbetas[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m0.9[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m0.999[39m[38;5;15m)[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mweight_decay[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m1e-4[39m
[38;5;15moptimizer[39m[38;5;204m.[39m[38;5;15mparams[39m[38;5;204m.[39m[38;5;15mlr_factor_func[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;81mlambda[39m[38;5;15m [39m[38;5;15mmodule_name[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m[38;5;15m [39m[38;5;81mif[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbackbone[39m[38;5;186m"[39m[38;5;15m [39m[38;5;204min[39m[38;5;15m [39m[38;5;15mmodule_name[39m[38;5;15m [39m[38;5;81melse[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;245m# modify dataloader config[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mnum_workers[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;245m# please notice that this is total batch size.[39m
[38;5;245m# surpose you're using 4 gpus for training and the batch size for[39m
[38;5;245m# each gpu is 16/4 = 4[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15mtotal_batch_size[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;245m# dump the testing results into output_dir for visualization[39m
[38;5;15mdataloader[39m[38;5;204m.[39m[38;5;15mevaluator[39m[38;5;204m.[39m[38;5;15moutput_dir[39m[38;5;15m [39m[38;5;204m=[39m[38;5;15m [39m[38;5;15mtrain[39m[38;5;204m.[39m[38;5;15moutput_dir[39m

WARNING [06/24 09:54:38 d2.config.lazy]: The config contains objects that cannot serialize to a valid yaml. ./output/stabledino_r50_4scale_12ep/config.yaml is human-readable but cannot be loaded.
WARNING [06/24 09:54:38 d2.config.lazy]: Config is saved using cloudpickle at ./output/stabledino_r50_4scale_12ep/config.yaml.pkl.
[06/24 09:54:38 detectron2]: Full config saved to ./output/stabledino_r50_4scale_12ep/config.yaml
[06/24 09:54:39 detectron2]: Model:
DINO(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (position_embedding): PositionEmbeddingSine()
  (neck): ChannelMapper(
    (convs): ModuleList(
      (0): ConvNormAct(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): ConvNormAct(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): ConvNormAct(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (extra_convs): ModuleList(
      (0): ConvNormAct(
        (conv): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
  )
  (transformer): DINOTransformer(
    (encoder): StableDINOTransformerEncoder(
      (layers): ModuleList(
        (0-5): 6 x BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiScaleDeformableAttention(
              (dropout): Dropout(p=0.0, inplace=False)
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0-1): 2 x LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (fusion_layer): Sequential(
        (0): Linear(in_features=1792, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (decoder): DINOTransformerDecoder(
      (layers): ModuleList(
        (0-5): 6 x BaseTransformerLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): MultiScaleDeformableAttention(
              (dropout): Dropout(p=0.0, inplace=False)
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activation): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (norms): ModuleList(
            (0-2): 3 x LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ref_point_head): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=512, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (class_embed): ModuleList(
        (0-6): 7 x Linear(in_features=256, out_features=80, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-6): 7 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
    )
    (tgt_embed): Embedding(900, 256)
    (enc_output): Linear(in_features=256, out_features=256, bias=True)
    (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (class_embed): ModuleList(
    (0-6): 7 x Linear(in_features=256, out_features=80, bias=True)
  )
  (bbox_embed): ModuleList(
    (0-6): 7 x MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (criterion): Criterion StableDINOCriterion
      matcher: Matcher StableDINOHungarianMatcher
          cost_class: 2.0
          cost_bbox: 5.0
          cost_giou: 2.0
          cost_class_type: focal_loss_cost
          focal cost alpha: 0.25
          focal cost gamma: 2.0
      losses: ['class', 'boxes']
      loss_class_type: focal_loss
      weight_dict: {'loss_class': 6.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_class_dn': 1, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_class_enc': 6.0, 'loss_bbox_enc': 5.0, 'loss_giou_enc': 2.0, 'loss_class_dn_enc': 1, 'loss_bbox_dn_enc': 5.0, 'loss_giou_dn_enc': 2.0, 'loss_class_0': 6.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_class_dn_0': 1, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_class_1': 6.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_class_dn_1': 1, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_class_2': 6.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_class_dn_2': 1, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_class_3': 6.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_class_dn_3': 1, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_class_4': 6.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_class_dn_4': 1, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_kd': -1.0}
      num_classes: 80
      eos_coef: None
      focal loss alpha: 0.25
      focal loss gamma: 2.0
  (label_enc): Embedding(80, 256)
)
[06/24 09:54:39 d2.data.datasets.coco]: Loaded 150 images in COCO format from /fs/pool/pool-lambacher/Cluster_Projects/data/coco/annotations/instances_train2017.json
[06/24 09:54:39 d2.data.build]: Removed 0 images with no usable annotations. 150 images left.
[06/24 09:54:39 d2.data.build]: Distribution of instances among all 80 categories:
|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 255          |   bicycle    | 0            |      car      | 61           |
|  motorcycle   | 6            |   airplane   | 3            |      bus      | 3            |
|     train     | 3            |    truck     | 6            |     boat      | 12           |
| traffic light | 14           | fire hydrant | 4            |   stop sign   | 3            |
| parking meter | 1            |    bench     | 6            |     bird      | 2            |
|      cat      | 14           |     dog      | 3            |     horse     | 15           |
|     sheep     | 9            |     cow      | 2            |   elephant    | 12           |
|     bear      | 3            |    zebra     | 6            |    giraffe    | 2            |
|   backpack    | 8            |   umbrella   | 10           |    handbag    | 13           |
|      tie      | 5            |   suitcase   | 8            |    frisbee    | 2            |
|     skis      | 7            |  snowboard   | 19           |  sports ball  | 8            |
|     kite      | 15           | baseball bat | 4            | baseball gl.. | 5            |
|  skateboard   | 3            |  surfboard   | 3            | tennis racket | 6            |
|    bottle     | 28           |  wine glass  | 7            |      cup      | 38           |
|     fork      | 8            |    knife     | 20           |     spoon     | 5            |
|     bowl      | 23           |    banana    | 17           |     apple     | 3            |
|   sandwich    | 1            |    orange    | 12           |   broccoli    | 3            |
|    carrot     | 1            |   hot dog    | 1            |     pizza     | 5            |
|     donut     | 2            |     cake     | 6            |     chair     | 34           |
|     couch     | 13           | potted plant | 29           |      bed      | 8            |
| dining table  | 15           |    toilet    | 7            |      tv       | 10           |
|    laptop     | 7            |    mouse     | 4            |    remote     | 15           |
|   keyboard    | 5            |  cell phone  | 7            |   microwave   | 3            |
|     oven      | 6            |   toaster    | 0            |     sink      | 11           |
| refrigerator  | 4            |     book     | 33           |     clock     | 8            |
|     vase      | 9            |   scissors   | 2            |  teddy bear   | 1            |
|  hair drier   | 2            |  toothbrush  | 3            |               |              |
|     total     | 977          |              |              |               |              |
[06/24 09:54:39 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...
[06/24 09:54:39 d2.data.common]: Serialized dataset takes 0.52 MiB
[06/24 09:54:39 fvcore.common.checkpoint]: [Checkpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[06/24 09:54:39 fvcore.common.checkpoint]: Reading a file from 'torchvision'
[06/24 09:54:39 d2.checkpoint.c2_model_loading]: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint                                                               | Shapes                                          |
|:------------------|:----------------------------------------------------------------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.*      | stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (64,) (64,) (64,) (64,) (64,3,7,7)              |
WARNING [06/24 09:54:39 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:
bbox_embed.0.layers.0.{bias, weight}
bbox_embed.0.layers.1.{bias, weight}
bbox_embed.0.layers.2.{bias, weight}
bbox_embed.1.layers.0.{bias, weight}
bbox_embed.1.layers.1.{bias, weight}
bbox_embed.1.layers.2.{bias, weight}
bbox_embed.2.layers.0.{bias, weight}
bbox_embed.2.layers.1.{bias, weight}
bbox_embed.2.layers.2.{bias, weight}
bbox_embed.3.layers.0.{bias, weight}
bbox_embed.3.layers.1.{bias, weight}
bbox_embed.3.layers.2.{bias, weight}
bbox_embed.4.layers.0.{bias, weight}
bbox_embed.4.layers.1.{bias, weight}
bbox_embed.4.layers.2.{bias, weight}
bbox_embed.5.layers.0.{bias, weight}
bbox_embed.5.layers.1.{bias, weight}
bbox_embed.5.layers.2.{bias, weight}
bbox_embed.6.layers.0.{bias, weight}
bbox_embed.6.layers.1.{bias, weight}
bbox_embed.6.layers.2.{bias, weight}
class_embed.0.{bias, weight}
class_embed.1.{bias, weight}
class_embed.2.{bias, weight}
class_embed.3.{bias, weight}
class_embed.4.{bias, weight}
class_embed.5.{bias, weight}
class_embed.6.{bias, weight}
label_enc.weight
neck.convs.0.conv.{bias, weight}
neck.convs.0.norm.{bias, weight}
neck.convs.1.conv.{bias, weight}
neck.convs.1.norm.{bias, weight}
neck.convs.2.conv.{bias, weight}
neck.convs.2.norm.{bias, weight}
neck.extra_convs.0.conv.{bias, weight}
neck.extra_convs.0.norm.{bias, weight}
transformer.decoder.bbox_embed.0.layers.0.{bias, weight}
transformer.decoder.bbox_embed.0.layers.1.{bias, weight}
transformer.decoder.bbox_embed.0.layers.2.{bias, weight}
transformer.decoder.bbox_embed.1.layers.0.{bias, weight}
transformer.decoder.bbox_embed.1.layers.1.{bias, weight}
transformer.decoder.bbox_embed.1.layers.2.{bias, weight}
transformer.decoder.bbox_embed.2.layers.0.{bias, weight}
transformer.decoder.bbox_embed.2.layers.1.{bias, weight}
transformer.decoder.bbox_embed.2.layers.2.{bias, weight}
transformer.decoder.bbox_embed.3.layers.0.{bias, weight}
transformer.decoder.bbox_embed.3.layers.1.{bias, weight}
transformer.decoder.bbox_embed.3.layers.2.{bias, weight}
transformer.decoder.bbox_embed.4.layers.0.{bias, weight}
transformer.decoder.bbox_embed.4.layers.1.{bias, weight}
transformer.decoder.bbox_embed.4.layers.2.{bias, weight}
transformer.decoder.bbox_embed.5.layers.0.{bias, weight}
transformer.decoder.bbox_embed.5.layers.1.{bias, weight}
transformer.decoder.bbox_embed.5.layers.2.{bias, weight}
transformer.decoder.bbox_embed.6.layers.0.{bias, weight}
transformer.decoder.bbox_embed.6.layers.1.{bias, weight}
transformer.decoder.bbox_embed.6.layers.2.{bias, weight}
transformer.decoder.class_embed.0.{bias, weight}
transformer.decoder.class_embed.1.{bias, weight}
transformer.decoder.class_embed.2.{bias, weight}
transformer.decoder.class_embed.3.{bias, weight}
transformer.decoder.class_embed.4.{bias, weight}
transformer.decoder.class_embed.5.{bias, weight}
transformer.decoder.class_embed.6.{bias, weight}
transformer.decoder.layers.0.attentions.0.attn.out_proj.{bias, weight}
transformer.decoder.layers.0.attentions.0.attn.{in_proj_bias, in_proj_weight}
transformer.decoder.layers.0.attentions.1.attention_weights.{bias, weight}
transformer.decoder.layers.0.attentions.1.output_proj.{bias, weight}
transformer.decoder.layers.0.attentions.1.sampling_offsets.{bias, weight}
transformer.decoder.layers.0.attentions.1.value_proj.{bias, weight}
transformer.decoder.layers.0.ffns.0.layers.0.0.{bias, weight}
transformer.decoder.layers.0.ffns.0.layers.1.{bias, weight}
transformer.decoder.layers.0.norms.0.{bias, weight}
transformer.decoder.layers.0.norms.1.{bias, weight}
transformer.decoder.layers.0.norms.2.{bias, weight}
transformer.decoder.layers.1.attentions.0.attn.out_proj.{bias, weight}
transformer.decoder.layers.1.attentions.0.attn.{in_proj_bias, in_proj_weight}
transformer.decoder.layers.1.attentions.1.attention_weights.{bias, weight}
transformer.decoder.layers.1.attentions.1.output_proj.{bias, weight}
transformer.decoder.layers.1.attentions.1.sampling_offsets.{bias, weight}
transformer.decoder.layers.1.attentions.1.value_proj.{bias, weight}
transformer.decoder.layers.1.ffns.0.layers.0.0.{bias, weight}
transformer.decoder.layers.1.ffns.0.layers.1.{bias, weight}
transformer.decoder.layers.1.norms.0.{bias, weight}
transformer.decoder.layers.1.norms.1.{bias, weight}
transformer.decoder.layers.1.norms.2.{bias, weight}
transformer.decoder.layers.2.attentions.0.attn.out_proj.{bias, weight}
transformer.decoder.layers.2.attentions.0.attn.{in_proj_bias, in_proj_weight}
transformer.decoder.layers.2.attentions.1.attention_weights.{bias, weight}
transformer.decoder.layers.2.attentions.1.output_proj.{bias, weight}
transformer.decoder.layers.2.attentions.1.sampling_offsets.{bias, weight}
transformer.decoder.layers.2.attentions.1.value_proj.{bias, weight}
transformer.decoder.layers.2.ffns.0.layers.0.0.{bias, weight}
transformer.decoder.layers.2.ffns.0.layers.1.{bias, weight}
transformer.decoder.layers.2.norms.0.{bias, weight}
transformer.decoder.layers.2.norms.1.{bias, weight}
transformer.decoder.layers.2.norms.2.{bias, weight}
transformer.decoder.layers.3.attentions.0.attn.out_proj.{bias, weight}
transformer.decoder.layers.3.attentions.0.attn.{in_proj_bias, in_proj_weight}
transformer.decoder.layers.3.attentions.1.attention_weights.{bias, weight}
transformer.decoder.layers.3.attentions.1.output_proj.{bias, weight}
transformer.decoder.layers.3.attentions.1.sampling_offsets.{bias, weight}
transformer.decoder.layers.3.attentions.1.value_proj.{bias, weight}
transformer.decoder.layers.3.ffns.0.layers.0.0.{bias, weight}
transformer.decoder.layers.3.ffns.0.layers.1.{bias, weight}
transformer.decoder.layers.3.norms.0.{bias, weight}
transformer.decoder.layers.3.norms.1.{bias, weight}
transformer.decoder.layers.3.norms.2.{bias, weight}
transformer.decoder.layers.4.attentions.0.attn.out_proj.{bias, weight}
transformer.decoder.layers.4.attentions.0.attn.{in_proj_bias, in_proj_weight}
transformer.decoder.layers.4.attentions.1.attention_weights.{bias, weight}
transformer.decoder.layers.4.attentions.1.output_proj.{bias, weight}
transformer.decoder.layers.4.attentions.1.sampling_offsets.{bias, weight}
transformer.decoder.layers.4.attentions.1.value_proj.{bias, weight}
transformer.decoder.layers.4.ffns.0.layers.0.0.{bias, weight}
transformer.decoder.layers.4.ffns.0.layers.1.{bias, weight}
transformer.decoder.layers.4.norms.0.{bias, weight}
transformer.decoder.layers.4.norms.1.{bias, weight}
transformer.decoder.layers.4.norms.2.{bias, weight}
transformer.decoder.layers.5.attentions.0.attn.out_proj.{bias, weight}
transformer.decoder.layers.5.attentions.0.attn.{in_proj_bias, in_proj_weight}
transformer.decoder.layers.5.attentions.1.attention_weights.{bias, weight}
transformer.decoder.layers.5.attentions.1.output_proj.{bias, weight}
transformer.decoder.layers.5.attentions.1.sampling_offsets.{bias, weight}
transformer.decoder.layers.5.attentions.1.value_proj.{bias, weight}
transformer.decoder.layers.5.ffns.0.layers.0.0.{bias, weight}
transformer.decoder.layers.5.ffns.0.layers.1.{bias, weight}
transformer.decoder.layers.5.norms.0.{bias, weight}
transformer.decoder.layers.5.norms.1.{bias, weight}
transformer.decoder.layers.5.norms.2.{bias, weight}
transformer.decoder.norm.{bias, weight}
transformer.decoder.ref_point_head.layers.0.{bias, weight}
transformer.decoder.ref_point_head.layers.1.{bias, weight}
transformer.enc_output.{bias, weight}
transformer.enc_output_norm.{bias, weight}
transformer.encoder.fusion_layer.0.{bias, weight}
transformer.encoder.fusion_layer.1.{bias, weight}
transformer.encoder.layers.0.attentions.0.attention_weights.{bias, weight}
transformer.encoder.layers.0.attentions.0.output_proj.{bias, weight}
transformer.encoder.layers.0.attentions.0.sampling_offsets.{bias, weight}
transformer.encoder.layers.0.attentions.0.value_proj.{bias, weight}
transformer.encoder.layers.0.ffns.0.layers.0.0.{bias, weight}
transformer.encoder.layers.0.ffns.0.layers.1.{bias, weight}
transformer.encoder.layers.0.norms.0.{bias, weight}
transformer.encoder.layers.0.norms.1.{bias, weight}
transformer.encoder.layers.1.attentions.0.attention_weights.{bias, weight}
transformer.encoder.layers.1.attentions.0.output_proj.{bias, weight}
transformer.encoder.layers.1.attentions.0.sampling_offsets.{bias, weight}
transformer.encoder.layers.1.attentions.0.value_proj.{bias, weight}
transformer.encoder.layers.1.ffns.0.layers.0.0.{bias, weight}
transformer.encoder.layers.1.ffns.0.layers.1.{bias, weight}
transformer.encoder.layers.1.norms.0.{bias, weight}
transformer.encoder.layers.1.norms.1.{bias, weight}
transformer.encoder.layers.2.attentions.0.attention_weights.{bias, weight}
transformer.encoder.layers.2.attentions.0.output_proj.{bias, weight}
transformer.encoder.layers.2.attentions.0.sampling_offsets.{bias, weight}
transformer.encoder.layers.2.attentions.0.value_proj.{bias, weight}
transformer.encoder.layers.2.ffns.0.layers.0.0.{bias, weight}
transformer.encoder.layers.2.ffns.0.layers.1.{bias, weight}
transformer.encoder.layers.2.norms.0.{bias, weight}
transformer.encoder.layers.2.norms.1.{bias, weight}
transformer.encoder.layers.3.attentions.0.attention_weights.{bias, weight}
transformer.encoder.layers.3.attentions.0.output_proj.{bias, weight}
transformer.encoder.layers.3.attentions.0.sampling_offsets.{bias, weight}
transformer.encoder.layers.3.attentions.0.value_proj.{bias, weight}
transformer.encoder.layers.3.ffns.0.layers.0.0.{bias, weight}
transformer.encoder.layers.3.ffns.0.layers.1.{bias, weight}
transformer.encoder.layers.3.norms.0.{bias, weight}
transformer.encoder.layers.3.norms.1.{bias, weight}
transformer.encoder.layers.4.attentions.0.attention_weights.{bias, weight}
transformer.encoder.layers.4.attentions.0.output_proj.{bias, weight}
transformer.encoder.layers.4.attentions.0.sampling_offsets.{bias, weight}
transformer.encoder.layers.4.attentions.0.value_proj.{bias, weight}
transformer.encoder.layers.4.ffns.0.layers.0.0.{bias, weight}
transformer.encoder.layers.4.ffns.0.layers.1.{bias, weight}
transformer.encoder.layers.4.norms.0.{bias, weight}
transformer.encoder.layers.4.norms.1.{bias, weight}
transformer.encoder.layers.5.attentions.0.attention_weights.{bias, weight}
transformer.encoder.layers.5.attentions.0.output_proj.{bias, weight}
transformer.encoder.layers.5.attentions.0.sampling_offsets.{bias, weight}
transformer.encoder.layers.5.attentions.0.value_proj.{bias, weight}
transformer.encoder.layers.5.ffns.0.layers.0.0.{bias, weight}
transformer.encoder.layers.5.ffns.0.layers.1.{bias, weight}
transformer.encoder.layers.5.norms.0.{bias, weight}
transformer.encoder.layers.5.norms.1.{bias, weight}
transformer.level_embeds
transformer.tgt_embed.weight
WARNING [06/24 09:54:39 fvcore.common.checkpoint]: The checkpoint state_dict contains keys that are not used by the model:
  stem.fc.{bias, weight}
[06/24 09:54:39 d2.engine.train_loop]: Starting training from iteration 0
[06/24 09:54:58 d2.utils.events]:  eta: 0:01:05  iter: 19  total_loss: 49.9  loss_class: 2.155  loss_bbox: 1.261  loss_giou: 1.397  loss_class_0: 2.113  loss_bbox_0: 1.278  loss_giou_0: 1.38  loss_class_1: 2.166  loss_bbox_1: 1.26  loss_giou_1: 1.387  loss_class_2: 2.23  loss_bbox_2: 1.26  loss_giou_2: 1.387  loss_class_3: 2.22  loss_bbox_3: 1.261  loss_giou_3: 1.391  loss_class_4: 2.198  loss_bbox_4: 1.266  loss_giou_4: 1.386  loss_class_enc: 2.236  loss_bbox_enc: 1.294  loss_giou_enc: 1.389  loss_class_dn: 0.09954  loss_bbox_dn: 1.119  loss_giou_dn: 1.304  loss_class_dn_0: 0.09286  loss_bbox_dn_0: 1.095  loss_giou_dn_0: 1.308  loss_class_dn_1: 0.09346  loss_bbox_dn_1: 1.099  loss_giou_dn_1: 1.306  loss_class_dn_2: 0.09605  loss_bbox_dn_2: 1.104  loss_giou_dn_2: 1.304  loss_class_dn_3: 0.09827  loss_bbox_dn_3: 1.109  loss_giou_dn_3: 1.303  loss_class_dn_4: 0.09612  loss_bbox_dn_4: 1.114  loss_giou_dn_4: 1.303  time: 0.5583  data_time: 0.3638  lr: 0.0001  max_mem: 17722M
[06/24 09:55:09 d2.utils.events]:  eta: 0:00:56  iter: 39  total_loss: 49.71  loss_class: 2.309  loss_bbox: 1.115  loss_giou: 1.315  loss_class_0: 2.279  loss_bbox_0: 1.221  loss_giou_0: 1.362  loss_class_1: 2.343  loss_bbox_1: 1.174  loss_giou_1: 1.325  loss_class_2: 2.42  loss_bbox_2: 1.163  loss_giou_2: 1.317  loss_class_3: 2.281  loss_bbox_3: 1.128  loss_giou_3: 1.325  loss_class_4: 2.273  loss_bbox_4: 1.123  loss_giou_4: 1.317  loss_class_enc: 2.051  loss_bbox_enc: 1.222  loss_giou_enc: 1.344  loss_class_dn: 0.09366  loss_bbox_dn: 1.171  loss_giou_dn: 1.331  loss_class_dn_0: 0.09077  loss_bbox_dn_0: 1.127  loss_giou_dn_0: 1.309  loss_class_dn_1: 0.09318  loss_bbox_dn_1: 1.135  loss_giou_dn_1: 1.31  loss_class_dn_2: 0.09163  loss_bbox_dn_2: 1.144  loss_giou_dn_2: 1.312  loss_class_dn_3: 0.09538  loss_bbox_dn_3: 1.154  loss_giou_dn_3: 1.321  loss_class_dn_4: 0.09029  loss_bbox_dn_4: 1.163  loss_giou_dn_4: 1.326  time: 0.5492  data_time: 0.0634  lr: 0.0001  max_mem: 20099M
[06/24 09:55:19 d2.utils.events]:  eta: 0:00:46  iter: 59  total_loss: 46.74  loss_class: 1.922  loss_bbox: 1.287  loss_giou: 1.311  loss_class_0: 1.796  loss_bbox_0: 1.345  loss_giou_0: 1.319  loss_class_1: 1.937  loss_bbox_1: 1.285  loss_giou_1: 1.308  loss_class_2: 1.924  loss_bbox_2: 1.278  loss_giou_2: 1.309  loss_class_3: 1.904  loss_bbox_3: 1.288  loss_giou_3: 1.312  loss_class_4: 1.912  loss_bbox_4: 1.285  loss_giou_4: 1.312  loss_class_enc: 1.752  loss_bbox_enc: 1.364  loss_giou_enc: 1.309  loss_class_dn: 0.09784  loss_bbox_dn: 1.303  loss_giou_dn: 1.296  loss_class_dn_0: 0.08895  loss_bbox_dn_0: 1.241  loss_giou_dn_0: 1.298  loss_class_dn_1: 0.09199  loss_bbox_dn_1: 1.262  loss_giou_dn_1: 1.295  loss_class_dn_2: 0.09529  loss_bbox_dn_2: 1.278  loss_giou_dn_2: 1.297  loss_class_dn_3: 0.09554  loss_bbox_dn_3: 1.29  loss_giou_dn_3: 1.295  loss_class_dn_4: 0.0971  loss_bbox_dn_4: 1.297  loss_giou_dn_4: 1.295  time: 0.5324  data_time: 0.0639  lr: 0.0001  max_mem: 20099M
[06/24 09:55:30 d2.utils.events]:  eta: 0:00:36  iter: 79  total_loss: 44.58  loss_class: 2.102  loss_bbox: 1.116  loss_giou: 1.315  loss_class_0: 1.703  loss_bbox_0: 1.164  loss_giou_0: 1.337  loss_class_1: 1.918  loss_bbox_1: 1.133  loss_giou_1: 1.32  loss_class_2: 2.013  loss_bbox_2: 1.117  loss_giou_2: 1.316  loss_class_3: 2.082  loss_bbox_3: 1.114  loss_giou_3: 1.31  loss_class_4: 2.098  loss_bbox_4: 1.13  loss_giou_4: 1.314  loss_class_enc: 1.637  loss_bbox_enc: 1.164  loss_giou_enc: 1.335  loss_class_dn: 0.09702  loss_bbox_dn: 1.015  loss_giou_dn: 1.32  loss_class_dn_0: 0.09005  loss_bbox_dn_0: 1.004  loss_giou_dn_0: 1.31  loss_class_dn_1: 0.09286  loss_bbox_dn_1: 1.009  loss_giou_dn_1: 1.317  loss_class_dn_2: 0.09649  loss_bbox_dn_2: 1.012  loss_giou_dn_2: 1.321  loss_class_dn_3: 0.09818  loss_bbox_dn_3: 1.014  loss_giou_dn_3: 1.321  loss_class_dn_4: 0.09904  loss_bbox_dn_4: 1.015  loss_giou_dn_4: 1.32  time: 0.5295  data_time: 0.0629  lr: 0.0001  max_mem: 20099M
[06/24 09:55:40 d2.utils.events]:  eta: 0:00:25  iter: 99  total_loss: 45.14  loss_class: 2.235  loss_bbox: 1.077  loss_giou: 1.232  loss_class_0: 2.026  loss_bbox_0: 1.105  loss_giou_0: 1.258  loss_class_1: 2.242  loss_bbox_1: 1.067  loss_giou_1: 1.237  loss_class_2: 2.272  loss_bbox_2: 1.065  loss_giou_2: 1.231  loss_class_3: 2.266  loss_bbox_3: 1.07  loss_giou_3: 1.231  loss_class_4: 2.252  loss_bbox_4: 1.068  loss_giou_4: 1.232  loss_class_enc: 1.648  loss_bbox_enc: 1.094  loss_giou_enc: 1.28  loss_class_dn: 0.09235  loss_bbox_dn: 1.091  loss_giou_dn: 1.303  loss_class_dn_0: 0.08109  loss_bbox_dn_0: 1.071  loss_giou_dn_0: 1.304  loss_class_dn_1: 0.08156  loss_bbox_dn_1: 1.082  loss_giou_dn_1: 1.301  loss_class_dn_2: 0.08854  loss_bbox_dn_2: 1.086  loss_giou_dn_2: 1.3  loss_class_dn_3: 0.09026  loss_bbox_dn_3: 1.088  loss_giou_dn_3: 1.3  loss_class_dn_4: 0.09166  loss_bbox_dn_4: 1.09  loss_giou_dn_4: 1.3  time: 0.5227  data_time: 0.0615  lr: 0.0001  max_mem: 20099M
[06/24 09:55:50 d2.utils.events]:  eta: 0:00:15  iter: 119  total_loss: 44.07  loss_class: 1.686  loss_bbox: 1.091  loss_giou: 1.278  loss_class_0: 1.781  loss_bbox_0: 1.133  loss_giou_0: 1.291  loss_class_1: 1.812  loss_bbox_1: 1.129  loss_giou_1: 1.273  loss_class_2: 1.84  loss_bbox_2: 1.085  loss_giou_2: 1.269  loss_class_3: 1.697  loss_bbox_3: 1.087  loss_giou_3: 1.273  loss_class_4: 1.667  loss_bbox_4: 1.083  loss_giou_4: 1.266  loss_class_enc: 1.548  loss_bbox_enc: 1.145  loss_giou_enc: 1.291  loss_class_dn: 0.08658  loss_bbox_dn: 1.064  loss_giou_dn: 1.303  loss_class_dn_0: 0.07898  loss_bbox_dn_0: 1.057  loss_giou_dn_0: 1.3  loss_class_dn_1: 0.08193  loss_bbox_dn_1: 1.059  loss_giou_dn_1: 1.299  loss_class_dn_2: 0.08581  loss_bbox_dn_2: 1.061  loss_giou_dn_2: 1.299  loss_class_dn_3: 0.08638  loss_bbox_dn_3: 1.062  loss_giou_dn_3: 1.298  loss_class_dn_4: 0.08902  loss_bbox_dn_4: 1.063  loss_giou_dn_4: 1.298  time: 0.5202  data_time: 0.0629  lr: 0.0001  max_mem: 20099M
[06/24 09:56:00 d2.utils.events]:  eta: 0:00:05  iter: 139  total_loss: 41.19  loss_class: 1.625  loss_bbox: 0.9574  loss_giou: 1.19  loss_class_0: 1.714  loss_bbox_0: 0.973  loss_giou_0: 1.223  loss_class_1: 1.643  loss_bbox_1: 0.9652  loss_giou_1: 1.203  loss_class_2: 1.629  loss_bbox_2: 0.9641  loss_giou_2: 1.183  loss_class_3: 1.649  loss_bbox_3: 0.9594  loss_giou_3: 1.193  loss_class_4: 1.661  loss_bbox_4: 0.9572  loss_giou_4: 1.192  loss_class_enc: 1.265  loss_bbox_enc: 1.102  loss_giou_enc: 1.24  loss_class_dn: 0.08503  loss_bbox_dn: 1.069  loss_giou_dn: 1.293  loss_class_dn_0: 0.07638  loss_bbox_dn_0: 1.064  loss_giou_dn_0: 1.29  loss_class_dn_1: 0.08233  loss_bbox_dn_1: 1.067  loss_giou_dn_1: 1.294  loss_class_dn_2: 0.0863  loss_bbox_dn_2: 1.068  loss_giou_dn_2: 1.295  loss_class_dn_3: 0.08616  loss_bbox_dn_3: 1.069  loss_giou_dn_3: 1.296  loss_class_dn_4: 0.08347  loss_bbox_dn_4: 1.068  loss_giou_dn_4: 1.295  time: 0.5168  data_time: 0.0619  lr: 1e-05  max_mem: 20099M
[06/24 09:56:05 fvcore.common.checkpoint]: Saving checkpoint to ./output/stabledino_r50_4scale_12ep/model_final.pth
[06/24 09:56:05 d2.utils.events]:  eta: 0:00:00  iter: 149  total_loss: 38.33  loss_class: 1.426  loss_bbox: 0.9104  loss_giou: 1.179  loss_class_0: 1.294  loss_bbox_0: 0.9626  loss_giou_0: 1.236  loss_class_1: 1.36  loss_bbox_1: 0.9145  loss_giou_1: 1.199  loss_class_2: 1.342  loss_bbox_2: 0.9133  loss_giou_2: 1.185  loss_class_3: 1.33  loss_bbox_3: 0.9078  loss_giou_3: 1.19  loss_class_4: 1.322  loss_bbox_4: 0.9128  loss_giou_4: 1.183  loss_class_enc: 1.265  loss_bbox_enc: 0.953  loss_giou_enc: 1.233  loss_class_dn: 0.08805  loss_bbox_dn: 0.9984  loss_giou_dn: 1.317  loss_class_dn_0: 0.07634  loss_bbox_dn_0: 0.9876  loss_giou_dn_0: 1.316  loss_class_dn_1: 0.08029  loss_bbox_dn_1: 0.9934  loss_giou_dn_1: 1.317  loss_class_dn_2: 0.08833  loss_bbox_dn_2: 0.9962  loss_giou_dn_2: 1.318  loss_class_dn_3: 0.08865  loss_bbox_dn_3: 0.9978  loss_giou_dn_3: 1.318  loss_class_dn_4: 0.0888  loss_bbox_dn_4: 0.9982  loss_giou_dn_4: 1.317  time: 0.5149  data_time: 0.0598  lr: 1e-05  max_mem: 20099M
[06/24 09:56:05 d2.engine.hooks]: Overall training speed: 148 iterations in 0:01:16 (0.5149 s / it)
[06/24 09:56:05 d2.engine.hooks]: Total training time: 0:01:16 (0:00:00 on hooks)
[06/24 09:56:05 d2.data.datasets.coco]: Loaded 50 images in COCO format from /fs/pool/pool-lambacher/Cluster_Projects/data/coco/annotations/instances_val2017.json
[06/24 09:56:05 d2.data.build]: Distribution of instances among all 80 categories:
|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 160          |   bicycle    | 0            |      car      | 4            |
|  motorcycle   | 0            |   airplane   | 0            |      bus      | 0            |
|     train     | 1            |    truck     | 4            |     boat      | 0            |
| traffic light | 0            | fire hydrant | 2            |   stop sign   | 1            |
| parking meter | 0            |    bench     | 6            |     bird      | 5            |
|      cat      | 2            |     dog      | 1            |     horse     | 9            |
|     sheep     | 10           |     cow      | 0            |   elephant    | 6            |
|     bear      | 0            |    zebra     | 0            |    giraffe    | 0            |
|   backpack    | 4            |   umbrella   | 1            |    handbag    | 8            |
|      tie      | 5            |   suitcase   | 1            |    frisbee    | 0            |
|     skis      | 14           |  snowboard   | 0            |  sports ball  | 5            |
|     kite      | 7            | baseball bat | 5            | baseball gl.. | 3            |
|  skateboard   | 8            |  surfboard   | 0            | tennis racket | 3            |
|    bottle     | 7            |  wine glass  | 9            |      cup      | 10           |
|     fork      | 10           |    knife     | 6            |     spoon     | 3            |
|     bowl      | 6            |    banana    | 2            |     apple     | 4            |
|   sandwich    | 2            |    orange    | 6            |   broccoli    | 1            |
|    carrot     | 2            |   hot dog    | 1            |     pizza     | 6            |
|     donut     | 13           |     cake     | 4            |     chair     | 33           |
|     couch     | 3            | potted plant | 4            |      bed      | 3            |
| dining table  | 15           |    toilet    | 1            |      tv       | 4            |
|    laptop     | 2            |    mouse     | 1            |    remote     | 5            |
|   keyboard    | 1            |  cell phone  | 2            |   microwave   | 0            |
|     oven      | 1            |   toaster    | 0            |     sink      | 1            |
| refrigerator  | 0            |     book     | 9            |     clock     | 1            |
|     vase      | 0            |   scissors   | 0            |  teddy bear   | 3            |
|  hair drier   | 0            |  toothbrush  | 0            |               |              |
|     total     | 456          |              |              |               |              |
[06/24 09:56:05 d2.data.common]: Serializing 50 elements to byte tensors and concatenating them all ...
[06/24 09:56:05 d2.data.common]: Serialized dataset takes 0.24 MiB
[06/24 09:56:05 d2.evaluation.evaluator]: Start inference on 50 batches
[06/24 09:56:08 d2.evaluation.evaluator]: Inference done 11/50. Dataloading: 0.0114 s/iter. Inference: 0.0635 s/iter. Eval: 0.0003 s/iter. Total: 0.0752 s/iter. ETA=0:00:02
[06/24 09:56:11 d2.evaluation.evaluator]: Total inference time: 0:00:03.222320 (0.071607 s / iter per device, on 1 devices)
[06/24 09:56:11 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:02 (0.055049 s / iter per device, on 1 devices)
[06/24 09:56:11 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...
[06/24 09:56:11 d2.evaluation.coco_evaluation]: Saving results to ./output/dino_r50_4scale_12ep/coco_instances_results.json
[06/24 09:56:11 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
[06/24 09:56:11 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*
[06/24 09:56:11 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.07 seconds.
[06/24 09:56:11 d2.evaluation.fast_eval_api]: Accumulating evaluation results...
[06/24 09:56:11 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.12 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.002
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.006
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.016
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.007
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.013
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.009
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.056
[06/24 09:56:11 d2.evaluation.coco_evaluation]: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.164 | 0.554  | 0.004  | 0.002 | 0.122 | 1.609 |
[06/24 09:56:11 d2.evaluation.coco_evaluation]: Per-category bbox AP: 
| category      | AP    | category     | AP    | category       | AP    |
|:--------------|:------|:-------------|:------|:---------------|:------|
| person        | 1.600 | bicycle      | nan   | car            | 0.000 |
| motorcycle    | nan   | airplane     | nan   | bus            | nan   |
| train         | 0.000 | truck        | 0.000 | boat           | nan   |
| traffic light | nan   | fire hydrant | 3.030 | stop sign      | 0.000 |
| parking meter | nan   | bench        | 0.000 | bird           | 0.000 |
| cat           | 0.000 | dog          | 0.000 | horse          | 1.092 |
| sheep         | 0.000 | cow          | nan   | elephant       | 3.567 |
| bear          | nan   | zebra        | nan   | giraffe        | nan   |
| backpack      | 0.000 | umbrella     | 0.000 | handbag        | 0.000 |
| tie           | 0.000 | suitcase     | 0.000 | frisbee        | nan   |
| skis          | 0.000 | snowboard    | nan   | sports ball    | 0.000 |
| kite          | 0.000 | baseball bat | 0.000 | baseball glove | 0.000 |
| skateboard    | 0.000 | surfboard    | nan   | tennis racket  | 0.000 |
| bottle        | 0.000 | wine glass   | 0.000 | cup            | 0.117 |
| fork          | 0.000 | knife        | 0.000 | spoon          | 0.000 |
| bowl          | 0.000 | banana       | 0.000 | apple          | 0.000 |
| sandwich      | 0.000 | orange       | 0.252 | broccoli       | 0.000 |
| carrot        | 0.000 | hot dog      | 0.000 | pizza          | 0.000 |
| donut         | 0.000 | cake         | 0.000 | chair          | 0.000 |
| couch         | 0.000 | potted plant | 0.000 | bed            | 0.000 |
| dining table  | 0.000 | toilet       | 0.000 | tv             | 0.000 |
| laptop        | 0.000 | mouse        | 0.000 | remote         | 0.000 |
| keyboard      | 0.000 | cell phone   | 0.000 | microwave      | nan   |
| oven          | 0.000 | toaster      | nan   | sink           | 0.000 |
| refrigerator  | nan   | book         | 0.000 | clock          | 0.000 |
| vase          | nan   | scissors     | nan   | teddy bear     | 0.000 |
| hair drier    | nan   | toothbrush   | nan   |                |       |
[06/24 09:56:11 d2.evaluation.testing]: copypaste: Task: bbox
[06/24 09:56:11 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl
[06/24 09:56:11 d2.evaluation.testing]: copypaste: 0.1637,0.5539,0.0039,0.0019,0.1219,1.6093
Running inference with NMS ...
Changing NMS threshold from -1.0 to 0.8
[06/24 09:56:11 d2.data.datasets.coco]: Loaded 50 images in COCO format from /fs/pool/pool-lambacher/Cluster_Projects/data/coco/annotations/instances_val2017.json
[06/24 09:56:11 d2.data.common]: Serializing 50 elements to byte tensors and concatenating them all ...
[06/24 09:56:11 d2.data.common]: Serialized dataset takes 0.24 MiB
[06/24 09:56:11 d2.evaluation.evaluator]: Start inference on 50 batches
[06/24 09:56:14 d2.evaluation.evaluator]: Inference done 11/50. Dataloading: 0.0112 s/iter. Inference: 0.0428 s/iter. Eval: 0.0005 s/iter. Total: 0.0544 s/iter. ETA=0:00:02
[06/24 09:56:16 d2.evaluation.evaluator]: Total inference time: 0:00:02.490912 (0.055354 s / iter per device, on 1 devices)
[06/24 09:56:16 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:01 (0.041715 s / iter per device, on 1 devices)
[06/24 09:56:16 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...
[06/24 09:56:16 d2.evaluation.coco_evaluation]: Saving results to ./output/dino_r50_4scale_12ep/coco_instances_results.json
[06/24 09:56:17 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
[06/24 09:56:17 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*
[06/24 09:56:17 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.06 seconds.
[06/24 09:56:17 d2.evaluation.fast_eval_api]: Accumulating evaluation results...
[06/24 09:56:17 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.12 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.002
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.007
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.017
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.008
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.013
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.010
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.054
[06/24 09:56:17 d2.evaluation.coco_evaluation]: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.188 | 0.665  | 0.003  | 0.001 | 0.124 | 1.722 |
[06/24 09:56:17 d2.evaluation.coco_evaluation]: Per-category bbox AP: 
| category      | AP    | category     | AP    | category       | AP    |
|:--------------|:------|:-------------|:------|:---------------|:------|
| person        | 2.292 | bicycle      | nan   | car            | 0.000 |
| motorcycle    | nan   | airplane     | nan   | bus            | nan   |
| train         | 0.000 | truck        | 0.000 | boat           | nan   |
| traffic light | nan   | fire hydrant | 3.030 | stop sign      | 0.000 |
| parking meter | nan   | bench        | 0.000 | bird           | 0.000 |
| cat           | 0.000 | dog          | 0.000 | horse          | 1.442 |
| sheep         | 0.000 | cow          | nan   | elephant       | 3.918 |
| bear          | nan   | zebra        | nan   | giraffe        | nan   |
| backpack      | 0.000 | umbrella     | 0.000 | handbag        | 0.000 |
| tie           | 0.000 | suitcase     | 0.000 | frisbee        | nan   |
| skis          | 0.000 | snowboard    | nan   | sports ball    | 0.000 |
| kite          | 0.000 | baseball bat | 0.000 | baseball glove | 0.000 |
| skateboard    | 0.000 | surfboard    | nan   | tennis racket  | 0.000 |
| bottle        | 0.000 | wine glass   | 0.000 | cup            | 0.119 |
| fork          | 0.000 | knife        | 0.000 | spoon          | 0.000 |
| bowl          | 0.000 | banana       | 0.000 | apple          | 0.000 |
| sandwich      | 0.000 | orange       | 0.295 | broccoli       | 0.000 |
| carrot        | 0.000 | hot dog      | 0.000 | pizza          | 0.000 |
| donut         | 0.000 | cake         | 0.000 | chair          | 0.000 |
| couch         | 0.000 | potted plant | 0.000 | bed            | 0.000 |
| dining table  | 0.000 | toilet       | 0.000 | tv             | 0.000 |
| laptop        | 0.000 | mouse        | 0.000 | remote         | 0.000 |
| keyboard      | 0.000 | cell phone   | 0.000 | microwave      | nan   |
| oven          | 0.000 | toaster      | nan   | sink           | 0.000 |
| refrigerator  | nan   | book         | 0.000 | clock          | 0.000 |
| vase          | nan   | scissors     | nan   | teddy bear     | 0.000 |
| hair drier    | nan   | toothbrush   | nan   |                |       |
Restoring NMS threshold from 0.8 to -1.0
[06/24 09:56:17 d2.evaluation.testing]: copypaste: Task: bbox_wnms
[06/24 09:56:17 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl
[06/24 09:56:17 d2.evaluation.testing]: copypaste: 0.1881,0.6653,0.0029,0.0014,0.1239,1.7223
